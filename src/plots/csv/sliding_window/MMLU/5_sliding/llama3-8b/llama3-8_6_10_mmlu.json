{
    "mmlu": {
        "acc,none": 0.26541803161942745,
        "acc_stderr,none": 0.0037080125722303812,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.2507970244420829,
        "acc_stderr,none": 0.006317797569613057,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.16666666666666666,
        "acc_stderr,none": 0.03333333333333337
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.23636363636363636,
        "acc_stderr,none": 0.03317505930009179
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.22549019607843138,
        "acc_stderr,none": 0.02933116229425173
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.2109704641350211,
        "acc_stderr,none": 0.02655837250266192
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.2892561983471074,
        "acc_stderr,none": 0.04139112727635464
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.2037037037037037,
        "acc_stderr,none": 0.03893542518824847
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.2822085889570552,
        "acc_stderr,none": 0.03536117886664743
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.27167630057803466,
        "acc_stderr,none": 0.023948512905468337
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.24692737430167597,
        "acc_stderr,none": 0.01442229220480886
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.2572347266881029,
        "acc_stderr,none": 0.024826171289250888
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.2777777777777778,
        "acc_stderr,none": 0.02492200116888633
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.2503259452411995,
        "acc_stderr,none": 0.01106415102716543
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.30409356725146197,
        "acc_stderr,none": 0.03528211258245231
    },
    "mmlu_other": {
        "acc,none": 0.2442870936594786,
        "acc_stderr,none": 0.007667708571753869,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.22,
        "acc_stderr,none": 0.041633319989322695
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.2641509433962264,
        "acc_stderr,none": 0.027134291628741702
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.3063583815028902,
        "acc_stderr,none": 0.03514942551267438
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.35,
        "acc_stderr,none": 0.047937248544110196
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.10762331838565023,
        "acc_stderr,none": 0.020799400082880008
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.1553398058252427,
        "acc_stderr,none": 0.035865947385739734
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.2264957264957265,
        "acc_stderr,none": 0.027421007295392926
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.2681992337164751,
        "acc_stderr,none": 0.015842430835269445
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.2647058823529412,
        "acc_stderr,none": 0.025261691219729487
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.2553191489361702,
        "acc_stderr,none": 0.026011992930902023
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.1948529411764706,
        "acc_stderr,none": 0.024060599423487414
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.2710843373493976,
        "acc_stderr,none": 0.034605799075530255
    },
    "mmlu_social_sciences": {
        "acc,none": 0.29411764705882354,
        "acc_stderr,none": 0.00818832077891559,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.20175438596491227,
        "acc_stderr,none": 0.037752050135836386
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.3383838383838384,
        "acc_stderr,none": 0.033711241426263035
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.36787564766839376,
        "acc_stderr,none": 0.034801756684660366
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.36153846153846153,
        "acc_stderr,none": 0.024359581465396987
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.29831932773109243,
        "acc_stderr,none": 0.02971914287634286
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.29174311926605506,
        "acc_stderr,none": 0.01948930096887654
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.2595419847328244,
        "acc_stderr,none": 0.03844876139785271
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.27941176470588236,
        "acc_stderr,none": 0.018152871051538816
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.3,
        "acc_stderr,none": 0.04389311454644286
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.2897959183673469,
        "acc_stderr,none": 0.02904308868330433
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.21393034825870647,
        "acc_stderr,none": 0.02899690969332893
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.21,
        "acc_stderr,none": 0.04093601807403326
    },
    "mmlu_stem": {
        "acc,none": 0.28005074532191565,
        "acc_stderr,none": 0.007929958809280767,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.3,
        "acc_stderr,none": 0.046056618647183814
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.2962962962962963,
        "acc_stderr,none": 0.03944624162501116
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.23026315789473684,
        "acc_stderr,none": 0.03426059424403165
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.25,
        "acc_stderr,none": 0.03621034121889507
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.35,
        "acc_stderr,none": 0.0479372485441102
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542126
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.28,
        "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.38235294117647056,
        "acc_stderr,none": 0.04835503696107224
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.27,
        "acc_stderr,none": 0.0446196043338474
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.19148936170212766,
        "acc_stderr,none": 0.025722149992637795
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.25517241379310346,
        "acc_stderr,none": 0.03632984052707842
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.25925925925925924,
        "acc_stderr,none": 0.022569897074918417
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.2967741935483871,
        "acc_stderr,none": 0.02598850079241189
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.2512315270935961,
        "acc_stderr,none": 0.030516530732694436
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.22,
        "acc_stderr,none": 0.0416333199893227
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.25925925925925924,
        "acc_stderr,none": 0.026719240783712163
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.31788079470198677,
        "acc_stderr,none": 0.038020397601079024
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.4722222222222222,
        "acc_stderr,none": 0.0340470532865388
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.17857142857142858,
        "acc_stderr,none": 0.036352091215778065
    }
}