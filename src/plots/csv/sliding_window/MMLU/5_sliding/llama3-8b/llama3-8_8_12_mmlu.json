{
    "mmlu": {
        "acc,none": 0.25637373593505197,
        "acc_stderr,none": 0.0036710209777994665,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.24017003188097769,
        "acc_stderr,none": 0.006226346611693185,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.3412698412698413,
        "acc_stderr,none": 0.042407993275749234
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.21818181818181817,
        "acc_stderr,none": 0.03225078108306289
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.27450980392156865,
        "acc_stderr,none": 0.03132179803083291
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.24050632911392406,
        "acc_stderr,none": 0.02782078198114968
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.21487603305785125,
        "acc_stderr,none": 0.03749492448709697
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.2037037037037037,
        "acc_stderr,none": 0.03893542518824847
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.22085889570552147,
        "acc_stderr,none": 0.03259177392742178
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.20809248554913296,
        "acc_stderr,none": 0.021855255263421802
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.2346368715083799,
        "acc_stderr,none": 0.014173044098303686
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.24437299035369775,
        "acc_stderr,none": 0.024406162094668882
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.25,
        "acc_stderr,none": 0.02409347123262133
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.2379400260756193,
        "acc_stderr,none": 0.01087570078769425
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.29239766081871343,
        "acc_stderr,none": 0.03488647713457923
    },
    "mmlu_other": {
        "acc,none": 0.2706791116832958,
        "acc_stderr,none": 0.007959919901503527,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.31,
        "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.3471698113207547,
        "acc_stderr,none": 0.02930010170554965
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.2138728323699422,
        "acc_stderr,none": 0.031265112061730424
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.26,
        "acc_stderr,none": 0.044084400227680794
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.26905829596412556,
        "acc_stderr,none": 0.029763779406874972
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.2621359223300971,
        "acc_stderr,none": 0.04354631077260595
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.18803418803418803,
        "acc_stderr,none": 0.025598193686652265
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.28,
        "acc_stderr,none": 0.045126085985421276
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.26309067688378035,
        "acc_stderr,none": 0.015745497169049046
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.2777777777777778,
        "acc_stderr,none": 0.025646863097137908
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.29432624113475175,
        "acc_stderr,none": 0.027187127011503796
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.2647058823529412,
        "acc_stderr,none": 0.026799562024887688
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.30120481927710846,
        "acc_stderr,none": 0.0357160923005348
    },
    "mmlu_social_sciences": {
        "acc,none": 0.23919402014949626,
        "acc_stderr,none": 0.007682510181039069,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.18421052631578946,
        "acc_stderr,none": 0.03646758875075566
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.25757575757575757,
        "acc_stderr,none": 0.03115626951964683
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.2694300518134715,
        "acc_stderr,none": 0.03201867122877794
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.2,
        "acc_stderr,none": 0.020280805062535726
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.3067226890756303,
        "acc_stderr,none": 0.02995382389188705
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.26972477064220185,
        "acc_stderr,none": 0.01902848671111545
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.2366412213740458,
        "acc_stderr,none": 0.0372767357559692
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.21241830065359477,
        "acc_stderr,none": 0.016547148636203147
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.20909090909090908,
        "acc_stderr,none": 0.038950910157241364
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.24081632653061225,
        "acc_stderr,none": 0.02737294220178816
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.23383084577114427,
        "acc_stderr,none": 0.029929415408348377
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.24,
        "acc_stderr,none": 0.042923469599092816
    },
    "mmlu_stem": {
        "acc,none": 0.28322232794164287,
        "acc_stderr,none": 0.007952458874085288,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.18,
        "acc_stderr,none": 0.03861229196653696
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.2518518518518518,
        "acc_stderr,none": 0.03749850709174022
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.2631578947368421,
        "acc_stderr,none": 0.03583496176361064
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.2986111111111111,
        "acc_stderr,none": 0.038270523579507554
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.35,
        "acc_stderr,none": 0.0479372485441102
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001974
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.37,
        "acc_stderr,none": 0.048523658709391
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.3627450980392157,
        "acc_stderr,none": 0.04784060704105654
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.17,
        "acc_stderr,none": 0.03775251680686371
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.28085106382978725,
        "acc_stderr,none": 0.029379170464124818
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.2689655172413793,
        "acc_stderr,none": 0.03695183311650232
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.21957671957671956,
        "acc_stderr,none": 0.021320018599770344
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.3225806451612903,
        "acc_stderr,none": 0.026593084516572277
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.21674876847290642,
        "acc_stderr,none": 0.028990331252516235
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.26,
        "acc_stderr,none": 0.04408440022768077
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.2777777777777778,
        "acc_stderr,none": 0.027309140588230175
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.2185430463576159,
        "acc_stderr,none": 0.03374235550425694
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.44907407407407407,
        "acc_stderr,none": 0.03392238405321617
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.26785714285714285,
        "acc_stderr,none": 0.04203277291467762
    }
}