{
    "mmlu": {
        "acc,none": 0.6429283577837915,
        "acc_stderr,none": 0.003797112248971266,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5804463336875664,
        "acc_stderr,none": 0.0067564030906351906,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.49206349206349204,
        "acc_stderr,none": 0.044715725362943486
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7696969696969697,
        "acc_stderr,none": 0.0328766675860349
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8333333333333334,
        "acc_stderr,none": 0.026156867523931062
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.8143459915611815,
        "acc_stderr,none": 0.025310495376944867
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.859504132231405,
        "acc_stderr,none": 0.031722334260021585
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.75,
        "acc_stderr,none": 0.04186091791394607
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.754601226993865,
        "acc_stderr,none": 0.03380939813943354
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.6763005780346821,
        "acc_stderr,none": 0.025190181327608422
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3754189944134078,
        "acc_stderr,none": 0.01619510424846353
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.7138263665594855,
        "acc_stderr,none": 0.02567025924218895
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7376543209876543,
        "acc_stderr,none": 0.024477222856135114
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.45632333767926986,
        "acc_stderr,none": 0.012721420501462546
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8187134502923976,
        "acc_stderr,none": 0.029547741687640038
    },
    "mmlu_other": {
        "acc,none": 0.7186997103315095,
        "acc_stderr,none": 0.007729279578036579,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.61,
        "acc_stderr,none": 0.04902071300001974
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7358490566037735,
        "acc_stderr,none": 0.02713429162874171
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6473988439306358,
        "acc_stderr,none": 0.03643037168958548
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.35,
        "acc_stderr,none": 0.047937248544110196
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6995515695067265,
        "acc_stderr,none": 0.030769352008229136
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8543689320388349,
        "acc_stderr,none": 0.0349260647662379
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8760683760683761,
        "acc_stderr,none": 0.02158649400128138
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036846
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8186462324393359,
        "acc_stderr,none": 0.013778693778464088
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7679738562091504,
        "acc_stderr,none": 0.024170840879340866
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4787234042553192,
        "acc_stderr,none": 0.029800481645628693
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.7316176470588235,
        "acc_stderr,none": 0.02691748122437723
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5481927710843374,
        "acc_stderr,none": 0.03874371556587953
    },
    "mmlu_social_sciences": {
        "acc,none": 0.758531036724082,
        "acc_stderr,none": 0.007538541478304443,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.43859649122807015,
        "acc_stderr,none": 0.04668000738510455
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.803030303030303,
        "acc_stderr,none": 0.02833560973246336
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8860103626943006,
        "acc_stderr,none": 0.02293514405391943
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6615384615384615,
        "acc_stderr,none": 0.02399150050031304
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.7394957983193278,
        "acc_stderr,none": 0.02851025151234193
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8440366972477065,
        "acc_stderr,none": 0.015555802713590151
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7633587786259542,
        "acc_stderr,none": 0.03727673575596915
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.7140522875816994,
        "acc_stderr,none": 0.018280485072954676
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7272727272727273,
        "acc_stderr,none": 0.04265792110940588
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.746938775510204,
        "acc_stderr,none": 0.027833023871399694
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8656716417910447,
        "acc_stderr,none": 0.024112678240900815
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.86,
        "acc_stderr,none": 0.0348735088019777
    },
    "mmlu_stem": {
        "acc,none": 0.5486837932128132,
        "acc_stderr,none": 0.008496669112677609,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.3,
        "acc_stderr,none": 0.046056618647183814
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.674074074074074,
        "acc_stderr,none": 0.040491220417025055
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6842105263157895,
        "acc_stderr,none": 0.0378272898086547
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7569444444444444,
        "acc_stderr,none": 0.03586879280080342
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.46,
        "acc_stderr,none": 0.05009082659620333
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.52,
        "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.38,
        "acc_stderr,none": 0.048783173121456316
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.4803921568627451,
        "acc_stderr,none": 0.04971358884367406
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.81,
        "acc_stderr,none": 0.03942772444036625
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.6127659574468085,
        "acc_stderr,none": 0.03184389265339526
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6482758620689655,
        "acc_stderr,none": 0.039792366374974096
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.4074074074074074,
        "acc_stderr,none": 0.025305906241590632
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7580645161290323,
        "acc_stderr,none": 0.024362599693031103
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5123152709359606,
        "acc_stderr,none": 0.035169204442208966
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.63,
        "acc_stderr,none": 0.048523658709390974
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.34074074074074073,
        "acc_stderr,none": 0.028897748741131133
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.4105960264900662,
        "acc_stderr,none": 0.04016689594849928
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.5555555555555556,
        "acc_stderr,none": 0.03388857118502325
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.5535714285714286,
        "acc_stderr,none": 0.04718471485219588
    }
}