{
    "mmlu": {
        "acc,none": 0.6341689218060106,
        "acc_stderr,none": 0.003811405428662477,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.575557917109458,
        "acc_stderr,none": 0.006742803727213943,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.42063492063492064,
        "acc_stderr,none": 0.04415438226743744
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7454545454545455,
        "acc_stderr,none": 0.03401506715249039
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8529411764705882,
        "acc_stderr,none": 0.024857478080250458
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.810126582278481,
        "acc_stderr,none": 0.025530100460233504
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8099173553719008,
        "acc_stderr,none": 0.035817969517092825
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7222222222222222,
        "acc_stderr,none": 0.043300437496507416
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7484662576687117,
        "acc_stderr,none": 0.03408997886857529
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.7312138728323699,
        "acc_stderr,none": 0.02386800326250011
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3575418994413408,
        "acc_stderr,none": 0.01602939447489489
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.707395498392283,
        "acc_stderr,none": 0.02583989833487798
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7283950617283951,
        "acc_stderr,none": 0.02474862449053737
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.45632333767926986,
        "acc_stderr,none": 0.012721420501462547
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8128654970760234,
        "acc_stderr,none": 0.029913127232368036
    },
    "mmlu_other": {
        "acc,none": 0.7145156099130995,
        "acc_stderr,none": 0.007771789853259818,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.58,
        "acc_stderr,none": 0.049604496374885836
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7471698113207547,
        "acc_stderr,none": 0.026749899771241214
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6358381502890174,
        "acc_stderr,none": 0.03669072477416907
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001974
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.7219730941704036,
        "acc_stderr,none": 0.030069584874494047
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8640776699029126,
        "acc_stderr,none": 0.03393295729761012
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8675213675213675,
        "acc_stderr,none": 0.02220930907316561
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.82,
        "acc_stderr,none": 0.03861229196653695
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.822477650063857,
        "acc_stderr,none": 0.013664230995834852
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7254901960784313,
        "acc_stderr,none": 0.025553169991826503
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4645390070921986,
        "acc_stderr,none": 0.02975238965742705
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.6948529411764706,
        "acc_stderr,none": 0.0279715413701706
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5662650602409639,
        "acc_stderr,none": 0.03858158940685517
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7409814754631134,
        "acc_stderr,none": 0.007709468118234739,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.4649122807017544,
        "acc_stderr,none": 0.046920083813689104
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7828282828282829,
        "acc_stderr,none": 0.02937661648494564
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8963730569948186,
        "acc_stderr,none": 0.021995311963644248
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.617948717948718,
        "acc_stderr,none": 0.024635549163908237
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.7436974789915967,
        "acc_stderr,none": 0.02835962087053395
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8275229357798165,
        "acc_stderr,none": 0.016197807956848043
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.732824427480916,
        "acc_stderr,none": 0.03880848301082396
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.696078431372549,
        "acc_stderr,none": 0.018607552131279837
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6636363636363637,
        "acc_stderr,none": 0.04525393596302506
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7183673469387755,
        "acc_stderr,none": 0.02879518557429127
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8656716417910447,
        "acc_stderr,none": 0.024112678240900822
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.85,
        "acc_stderr,none": 0.035887028128263686
    },
    "mmlu_stem": {
        "acc,none": 0.5382175705677132,
        "acc_stderr,none": 0.00846381735810368,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.29,
        "acc_stderr,none": 0.045604802157206845
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.674074074074074,
        "acc_stderr,none": 0.040491220417025055
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6644736842105263,
        "acc_stderr,none": 0.038424985593952694
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7708333333333334,
        "acc_stderr,none": 0.03514697467862388
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.41,
        "acc_stderr,none": 0.049431107042371025
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.51,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.3,
        "acc_stderr,none": 0.046056618647183814
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.4803921568627451,
        "acc_stderr,none": 0.04971358884367405
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.82,
        "acc_stderr,none": 0.038612291966536934
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5617021276595745,
        "acc_stderr,none": 0.03243618636108101
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6551724137931034,
        "acc_stderr,none": 0.03960933549451207
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.3915343915343915,
        "acc_stderr,none": 0.025138091388851095
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7806451612903226,
        "acc_stderr,none": 0.02354079935872331
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.4827586206896552,
        "acc_stderr,none": 0.035158955511657
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.04725815626252607
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.3814814814814815,
        "acc_stderr,none": 0.029616718927497596
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.39072847682119205,
        "acc_stderr,none": 0.039837983066598075
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.5324074074074074,
        "acc_stderr,none": 0.03402801581358966
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.4732142857142857,
        "acc_stderr,none": 0.047389751192741546
    }
}