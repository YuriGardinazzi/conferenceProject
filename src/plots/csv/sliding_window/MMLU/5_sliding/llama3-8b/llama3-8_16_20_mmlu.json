{
    "mmlu": {
        "acc,none": 0.3223899729383279,
        "acc_stderr,none": 0.00391480878028663,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.2962805526036132,
        "acc_stderr,none": 0.0065770169062951455,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.2222222222222222,
        "acc_stderr,none": 0.037184890068181146
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.26666666666666666,
        "acc_stderr,none": 0.03453131801885415
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.35294117647058826,
        "acc_stderr,none": 0.03354092437591519
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.379746835443038,
        "acc_stderr,none": 0.031591887529658504
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.38016528925619836,
        "acc_stderr,none": 0.04431324501968432
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.42592592592592593,
        "acc_stderr,none": 0.0478034362693679
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.36809815950920244,
        "acc_stderr,none": 0.03789213935838396
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.3901734104046243,
        "acc_stderr,none": 0.026261677607806642
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.18324022346368715,
        "acc_stderr,none": 0.012938645613066388
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.3762057877813505,
        "acc_stderr,none": 0.02751392568354943
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.3765432098765432,
        "acc_stderr,none": 0.02695934451874779
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.27835723598435463,
        "acc_stderr,none": 0.011446990197380985
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.25146198830409355,
        "acc_stderr,none": 0.033275044238468436
    },
    "mmlu_other": {
        "acc,none": 0.3389121338912134,
        "acc_stderr,none": 0.008444057680752539,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.32075471698113206,
        "acc_stderr,none": 0.028727502957880263
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.2832369942196532,
        "acc_stderr,none": 0.03435568056047875
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.31,
        "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.515695067264574,
        "acc_stderr,none": 0.0335412657542081
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.3592233009708738,
        "acc_stderr,none": 0.04750458399041694
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.3162393162393162,
        "acc_stderr,none": 0.030463656747340247
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001975
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.3614303959131545,
        "acc_stderr,none": 0.01717960132890074
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.3366013071895425,
        "acc_stderr,none": 0.027057974624494382
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.2907801418439716,
        "acc_stderr,none": 0.027090664368353178
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.2757352941176471,
        "acc_stderr,none": 0.027146271936625166
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.3132530120481928,
        "acc_stderr,none": 0.036108050180310235
    },
    "mmlu_social_sciences": {
        "acc,none": 0.35196620084497887,
        "acc_stderr,none": 0.008607738650969412,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.3157894736842105,
        "acc_stderr,none": 0.04372748290278007
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.29797979797979796,
        "acc_stderr,none": 0.03258630383836556
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.3471502590673575,
        "acc_stderr,none": 0.034356961683613546
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.3435897435897436,
        "acc_stderr,none": 0.024078696580635477
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.36134453781512604,
        "acc_stderr,none": 0.03120469122515001
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.3431192660550459,
        "acc_stderr,none": 0.020354777736086037
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.29770992366412213,
        "acc_stderr,none": 0.040103589424622034
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.3660130718954248,
        "acc_stderr,none": 0.019488025745529675
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.33636363636363636,
        "acc_stderr,none": 0.04525393596302506
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.3551020408163265,
        "acc_stderr,none": 0.030635655150387638
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.40298507462686567,
        "acc_stderr,none": 0.034683432951111266
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.46,
        "acc_stderr,none": 0.05009082659620333
    },
    "mmlu_stem": {
        "acc,none": 0.3162067871868062,
        "acc_stderr,none": 0.008237252801645072,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.26,
        "acc_stderr,none": 0.04408440022768079
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.26666666666666666,
        "acc_stderr,none": 0.038201699145179055
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.375,
        "acc_stderr,none": 0.039397364351956274
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.2847222222222222,
        "acc_stderr,none": 0.03773809990686934
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.34,
        "acc_stderr,none": 0.04760952285695235
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.24,
        "acc_stderr,none": 0.04292346959909282
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.27450980392156865,
        "acc_stderr,none": 0.044405219061793254
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.36,
        "acc_stderr,none": 0.048241815132442176
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.3617021276595745,
        "acc_stderr,none": 0.03141082197596241
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.2896551724137931,
        "acc_stderr,none": 0.03780019230438014
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.2566137566137566,
        "acc_stderr,none": 0.022494510767503154
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.4064516129032258,
        "acc_stderr,none": 0.027941727346256308
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.32019704433497537,
        "acc_stderr,none": 0.032826493853041504
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.33,
        "acc_stderr,none": 0.047258156262526045
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.27037037037037037,
        "acc_stderr,none": 0.027080372815145658
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.23841059602649006,
        "acc_stderr,none": 0.03479185572599661
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.4305555555555556,
        "acc_stderr,none": 0.03376922151252335
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.33035714285714285,
        "acc_stderr,none": 0.04464285714285713
    }
}