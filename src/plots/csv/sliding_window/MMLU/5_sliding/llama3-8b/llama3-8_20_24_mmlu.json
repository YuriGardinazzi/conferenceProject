{
    "mmlu": {
        "acc,none": 0.5499928785073351,
        "acc_stderr,none": 0.003931609852385492,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.49819341126461214,
        "acc_stderr,none": 0.0069183636176946995,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.373015873015873,
        "acc_stderr,none": 0.04325506042017086
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7090909090909091,
        "acc_stderr,none": 0.03546563019624336
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7450980392156863,
        "acc_stderr,none": 0.030587591351604246
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7383966244725738,
        "acc_stderr,none": 0.028609516716994927
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.743801652892562,
        "acc_stderr,none": 0.03984979653302872
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.6296296296296297,
        "acc_stderr,none": 0.04668408033024931
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.6319018404907976,
        "acc_stderr,none": 0.03789213935838396
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.569364161849711,
        "acc_stderr,none": 0.026658800273672376
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.37206703910614525,
        "acc_stderr,none": 0.016165847583563292
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.5916398713826366,
        "acc_stderr,none": 0.027917050748484627
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.6574074074074074,
        "acc_stderr,none": 0.026406145973625658
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.3494132985658409,
        "acc_stderr,none": 0.012177306252786672
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7543859649122807,
        "acc_stderr,none": 0.0330140594698725
    },
    "mmlu_other": {
        "acc,none": 0.6401673640167364,
        "acc_stderr,none": 0.00820204953545028,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.59,
        "acc_stderr,none": 0.04943110704237101
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6528301886792452,
        "acc_stderr,none": 0.029300101705549645
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5606936416184971,
        "acc_stderr,none": 0.03784271932887467
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.2,
        "acc_stderr,none": 0.04020151261036845
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6771300448430493,
        "acc_stderr,none": 0.031381476375754995
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7864077669902912,
        "acc_stderr,none": 0.040580420156460344
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8205128205128205,
        "acc_stderr,none": 0.02514093595033544
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.71,
        "acc_stderr,none": 0.045604802157206845
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.7445721583652618,
        "acc_stderr,none": 0.015594955384455765
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.673202614379085,
        "acc_stderr,none": 0.026857294663281423
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.32978723404255317,
        "acc_stderr,none": 0.028045946942042405
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.6360294117647058,
        "acc_stderr,none": 0.029227192460032025
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5421686746987951,
        "acc_stderr,none": 0.038786267710023595
    },
    "mmlu_social_sciences": {
        "acc,none": 0.6694832629184271,
        "acc_stderr,none": 0.008242058953572518,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.3684210526315789,
        "acc_stderr,none": 0.04537815354939391
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7525252525252525,
        "acc_stderr,none": 0.03074630074212451
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8341968911917098,
        "acc_stderr,none": 0.026839845022314415
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5205128205128206,
        "acc_stderr,none": 0.02532966316348994
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.592436974789916,
        "acc_stderr,none": 0.03191863374478466
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7761467889908257,
        "acc_stderr,none": 0.017871217767790232
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.6717557251908397,
        "acc_stderr,none": 0.041184385658062976
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6225490196078431,
        "acc_stderr,none": 0.019610851474880276
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6454545454545455,
        "acc_stderr,none": 0.04582004841505417
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.6489795918367347,
        "acc_stderr,none": 0.030555316755573637
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7960199004975125,
        "acc_stderr,none": 0.02849317624532608
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.82,
        "acc_stderr,none": 0.038612291966536934
    },
    "mmlu_stem": {
        "acc,none": 0.42182048842372344,
        "acc_stderr,none": 0.008365437012954512,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.562962962962963,
        "acc_stderr,none": 0.042849586397534
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.5394736842105263,
        "acc_stderr,none": 0.04056242252249033
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7013888888888888,
        "acc_stderr,none": 0.038270523579507554
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145632
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.37,
        "acc_stderr,none": 0.04852365870939099
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.23,
        "acc_stderr,none": 0.042295258468165065
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.30392156862745096,
        "acc_stderr,none": 0.04576665403207762
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.69,
        "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.425531914893617,
        "acc_stderr,none": 0.03232146916224468
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.4413793103448276,
        "acc_stderr,none": 0.04137931034482758
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.2857142857142857,
        "acc_stderr,none": 0.023266512213730585
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.6903225806451613,
        "acc_stderr,none": 0.026302774983517414
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.4088669950738916,
        "acc_stderr,none": 0.034590588158832314
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.55,
        "acc_stderr,none": 0.04999999999999999
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.24444444444444444,
        "acc_stderr,none": 0.02620276653465215
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.23841059602649006,
        "acc_stderr,none": 0.03479185572599661
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.35185185185185186,
        "acc_stderr,none": 0.032568505702936464
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.4107142857142857,
        "acc_stderr,none": 0.04669510663875191
    }
}