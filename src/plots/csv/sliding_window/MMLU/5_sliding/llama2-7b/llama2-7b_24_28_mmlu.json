{
    "mmlu": {
        "acc,none": 0.44979347671271896,
        "acc_stderr,none": 0.0040878117896607,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.4303931987247609,
        "acc_stderr,none": 0.007064989685260811,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.3888888888888889,
        "acc_stderr,none": 0.04360314860077459
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.5878787878787879,
        "acc_stderr,none": 0.03843566993588717
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.4803921568627451,
        "acc_stderr,none": 0.03506612560524866
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.4978902953586498,
        "acc_stderr,none": 0.03254693801802008
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.628099173553719,
        "acc_stderr,none": 0.044120158066245044
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.4722222222222222,
        "acc_stderr,none": 0.04826217294139894
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.49079754601226994,
        "acc_stderr,none": 0.039277056007874414
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.48554913294797686,
        "acc_stderr,none": 0.026907849856282532
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.33631284916201115,
        "acc_stderr,none": 0.015801003729145887
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.5787781350482315,
        "acc_stderr,none": 0.02804339985821063
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.48148148148148145,
        "acc_stderr,none": 0.027801656212323674
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.34615384615384615,
        "acc_stderr,none": 0.012150699768228558
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7017543859649122,
        "acc_stderr,none": 0.03508771929824563
    },
    "mmlu_other": {
        "acc,none": 0.5111039588027035,
        "acc_stderr,none": 0.008722417646296034,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.5,
        "acc_stderr,none": 0.050251890762960605
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.4679245283018868,
        "acc_stderr,none": 0.03070948699255655
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.43352601156069365,
        "acc_stderr,none": 0.03778621079092055
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.22,
        "acc_stderr,none": 0.041633319989322695
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.5246636771300448,
        "acc_stderr,none": 0.03351695167652628
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.5436893203883495,
        "acc_stderr,none": 0.049318019942204146
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.6923076923076923,
        "acc_stderr,none": 0.030236389942173095
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.56,
        "acc_stderr,none": 0.04988876515698589
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.6309067688378033,
        "acc_stderr,none": 0.017256283109124613
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.4934640522875817,
        "acc_stderr,none": 0.02862747055055606
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.29432624113475175,
        "acc_stderr,none": 0.02718712701150379
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.4852941176470588,
        "acc_stderr,none": 0.03035969707904612
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.39759036144578314,
        "acc_stderr,none": 0.038099730845402184
    },
    "mmlu_social_sciences": {
        "acc,none": 0.49983750406239846,
        "acc_stderr,none": 0.008852192098257305,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.22807017543859648,
        "acc_stderr,none": 0.03947152782669415
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.5151515151515151,
        "acc_stderr,none": 0.03560716516531061
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.6580310880829016,
        "acc_stderr,none": 0.03423465100104283
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.45897435897435895,
        "acc_stderr,none": 0.025265525491284295
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.3907563025210084,
        "acc_stderr,none": 0.031693802357129965
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.5944954128440367,
        "acc_stderr,none": 0.021050997991896837
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.5725190839694656,
        "acc_stderr,none": 0.04338920305792401
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.43300653594771243,
        "acc_stderr,none": 0.020045442473324224
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.4909090909090909,
        "acc_stderr,none": 0.04788339768702861
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.44081632653061226,
        "acc_stderr,none": 0.03178419114175363
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.5920398009950248,
        "acc_stderr,none": 0.03475116365194092
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.66,
        "acc_stderr,none": 0.04760952285695238
    },
    "mmlu_stem": {
        "acc,none": 0.3694893751982239,
        "acc_stderr,none": 0.008471847522389801,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.32,
        "acc_stderr,none": 0.046882617226215034
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.43703703703703706,
        "acc_stderr,none": 0.04284958639753399
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.4342105263157895,
        "acc_stderr,none": 0.0403356566784832
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.4722222222222222,
        "acc_stderr,none": 0.04174752578923185
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.44,
        "acc_stderr,none": 0.0498887651569859
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.35,
        "acc_stderr,none": 0.047937248544110196
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.26,
        "acc_stderr,none": 0.04408440022768077
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.27450980392156865,
        "acc_stderr,none": 0.04440521906179326
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.57,
        "acc_stderr,none": 0.04975698519562428
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.3872340425531915,
        "acc_stderr,none": 0.03184389265339526
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.4206896551724138,
        "acc_stderr,none": 0.0411391498118926
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.2751322751322751,
        "acc_stderr,none": 0.02300008685906864
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.5064516129032258,
        "acc_stderr,none": 0.02844163823354051
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.33004926108374383,
        "acc_stderr,none": 0.03308530426228257
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.47,
        "acc_stderr,none": 0.050161355804659205
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.25925925925925924,
        "acc_stderr,none": 0.026719240783712166
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.2913907284768212,
        "acc_stderr,none": 0.03710185726119994
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.33796296296296297,
        "acc_stderr,none": 0.032259413526312945
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.32142857142857145,
        "acc_stderr,none": 0.0443280405529152
    }
}