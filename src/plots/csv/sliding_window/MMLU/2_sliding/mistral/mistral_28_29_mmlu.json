{
    "mmlu": {
        "acc,none": 0.5896595926506196,
        "acc_stderr,none": 0.0038554668721370224,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5207226354941552,
        "acc_stderr,none": 0.006624724879569689,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.2857142857142857,
        "acc_stderr,none": 0.040406101782088394
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7636363636363637,
        "acc_stderr,none": 0.033175059300091805
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7794117647058824,
        "acc_stderr,none": 0.02910225438967409
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7468354430379747,
        "acc_stderr,none": 0.0283046579430353
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8181818181818182,
        "acc_stderr,none": 0.03520893951097652
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7314814814814815,
        "acc_stderr,none": 0.042844679680521934
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.754601226993865,
        "acc_stderr,none": 0.03380939813943354
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.6965317919075145,
        "acc_stderr,none": 0.024752411960917205
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.26033519553072626,
        "acc_stderr,none": 0.014676252009319471
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.684887459807074,
        "acc_stderr,none": 0.026385273703464496
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7191358024691358,
        "acc_stderr,none": 0.025006469755799204
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.3833116036505867,
        "acc_stderr,none": 0.012417603662901188
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8362573099415205,
        "acc_stderr,none": 0.028380919596145866
    },
    "mmlu_other": {
        "acc,none": 0.6704216285806244,
        "acc_stderr,none": 0.008057475875437713,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.57,
        "acc_stderr,none": 0.04975698519562428
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6528301886792452,
        "acc_stderr,none": 0.02930010170554965
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5953757225433526,
        "acc_stderr,none": 0.03742461193887249
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.34,
        "acc_stderr,none": 0.04760952285695235
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.695067264573991,
        "acc_stderr,none": 0.030898610882477515
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8155339805825242,
        "acc_stderr,none": 0.03840423627288276
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8632478632478633,
        "acc_stderr,none": 0.022509033937077812
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.72,
        "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.80970625798212,
        "acc_stderr,none": 0.014036945850381401
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.6895424836601307,
        "acc_stderr,none": 0.0264930332251459
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.44680851063829785,
        "acc_stderr,none": 0.029658235097666907
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5183823529411765,
        "acc_stderr,none": 0.030352303395351964
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5481927710843374,
        "acc_stderr,none": 0.03874371556587953
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7065323366915827,
        "acc_stderr,none": 0.008054195388477183,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.45614035087719296,
        "acc_stderr,none": 0.04685473041907789
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7626262626262627,
        "acc_stderr,none": 0.0303137105381989
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8393782383419689,
        "acc_stderr,none": 0.026499057701397433
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6051282051282051,
        "acc_stderr,none": 0.02478431694215638
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6008403361344538,
        "acc_stderr,none": 0.03181110032413926
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7706422018348624,
        "acc_stderr,none": 0.018025349724618684
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.8015267175572519,
        "acc_stderr,none": 0.03498149385462472
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6683006535947712,
        "acc_stderr,none": 0.019047485239360385
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6818181818181818,
        "acc_stderr,none": 0.044612721759105085
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7142857142857143,
        "acc_stderr,none": 0.028920583220675568
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7960199004975125,
        "acc_stderr,none": 0.028493176245326088
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.86,
        "acc_stderr,none": 0.03487350880197768
    },
    "mmlu_stem": {
        "acc,none": 0.49888994608309545,
        "acc_stderr,none": 0.0085023625042195,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.27,
        "acc_stderr,none": 0.044619604333847394
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6148148148148148,
        "acc_stderr,none": 0.042039210401562783
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6381578947368421,
        "acc_stderr,none": 0.03910525752849724
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7152777777777778,
        "acc_stderr,none": 0.03773809990686935
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.44,
        "acc_stderr,none": 0.04988876515698589
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.49,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.32,
        "acc_stderr,none": 0.04688261722621504
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.29411764705882354,
        "acc_stderr,none": 0.04533838195929774
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036846
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5574468085106383,
        "acc_stderr,none": 0.03246956919789958
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.5862068965517241,
        "acc_stderr,none": 0.04104269211806232
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.3968253968253968,
        "acc_stderr,none": 0.02519710107424649
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7161290322580646,
        "acc_stderr,none": 0.02564938106302926
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5073891625615764,
        "acc_stderr,none": 0.0351760354036101
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.047258156262526066
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.32222222222222224,
        "acc_stderr,none": 0.028493465091028593
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.31125827814569534,
        "acc_stderr,none": 0.03780445850526733
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.37962962962962965,
        "acc_stderr,none": 0.03309682581119035
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.48214285714285715,
        "acc_stderr,none": 0.047427623612430116
    }
}