{
    "mmlu": {
        "acc,none": 0.6162227602905569,
        "acc_stderr,none": 0.003853459859241936,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5642933049946866,
        "acc_stderr,none": 0.006756458769667531,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.4126984126984127,
        "acc_stderr,none": 0.04403438954768177
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7696969696969697,
        "acc_stderr,none": 0.0328766675860349
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7892156862745098,
        "acc_stderr,none": 0.0286265479124374
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7426160337552743,
        "acc_stderr,none": 0.028458820991460288
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8016528925619835,
        "acc_stderr,none": 0.03640118271990947
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7407407407407407,
        "acc_stderr,none": 0.04236511258094632
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7730061349693251,
        "acc_stderr,none": 0.03291099578615771
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.7052023121387283,
        "acc_stderr,none": 0.02454761779480384
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3094972067039106,
        "acc_stderr,none": 0.015461169002371546
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.6881028938906752,
        "acc_stderr,none": 0.02631185807185416
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7253086419753086,
        "acc_stderr,none": 0.024836057868294677
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.47131681877444587,
        "acc_stderr,none": 0.012749206007657467
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8362573099415205,
        "acc_stderr,none": 0.028380919596145866
    },
    "mmlu_other": {
        "acc,none": 0.6926295461860316,
        "acc_stderr,none": 0.007938897597400858,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.59,
        "acc_stderr,none": 0.04943110704237102
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6679245283018868,
        "acc_stderr,none": 0.028985455652334395
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6242774566473989,
        "acc_stderr,none": 0.03692820767264867
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.27,
        "acc_stderr,none": 0.044619604333847394
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6905829596412556,
        "acc_stderr,none": 0.031024411740572223
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7961165048543689,
        "acc_stderr,none": 0.039891398595317706
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8632478632478633,
        "acc_stderr,none": 0.0225090339370778
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.73,
        "acc_stderr,none": 0.044619604333847415
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8135376756066411,
        "acc_stderr,none": 0.013927751372001506
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7320261437908496,
        "acc_stderr,none": 0.025360603796242557
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4858156028368794,
        "acc_stderr,none": 0.02981549448368206
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.6654411764705882,
        "acc_stderr,none": 0.028661996202335307
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5481927710843374,
        "acc_stderr,none": 0.03874371556587953
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7214819629509263,
        "acc_stderr,none": 0.007916133476118534,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.47368421052631576,
        "acc_stderr,none": 0.046970851366478626
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7676767676767676,
        "acc_stderr,none": 0.030088629490217487
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8652849740932642,
        "acc_stderr,none": 0.024639789097709443
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6282051282051282,
        "acc_stderr,none": 0.024503472557110936
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6302521008403361,
        "acc_stderr,none": 0.031357095996135904
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7981651376146789,
        "acc_stderr,none": 0.01720857935778755
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7938931297709924,
        "acc_stderr,none": 0.03547771004159463
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.673202614379085,
        "acc_stderr,none": 0.018975427920507215
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6545454545454545,
        "acc_stderr,none": 0.04554619617541054
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7142857142857143,
        "acc_stderr,none": 0.028920583220675578
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8258706467661692,
        "acc_stderr,none": 0.026814951200421603
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.88,
        "acc_stderr,none": 0.03265986323710906
    },
    "mmlu_stem": {
        "acc,none": 0.5156993339676499,
        "acc_stderr,none": 0.008484942977966703,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6296296296296297,
        "acc_stderr,none": 0.041716541613545426
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6513157894736842,
        "acc_stderr,none": 0.0387813988879761
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7222222222222222,
        "acc_stderr,none": 0.037455547914624576
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.46,
        "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.52,
        "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145633
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.3137254901960784,
        "acc_stderr,none": 0.04617034827006718
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.77,
        "acc_stderr,none": 0.04229525846816505
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5446808510638298,
        "acc_stderr,none": 0.03255525359340355
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.5724137931034483,
        "acc_stderr,none": 0.041227371113703344
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.373015873015873,
        "acc_stderr,none": 0.02490699045899257
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7548387096774194,
        "acc_stderr,none": 0.024472243840895528
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.4827586206896552,
        "acc_stderr,none": 0.035158955511657
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.04725815626252607
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.34814814814814815,
        "acc_stderr,none": 0.029045600290616255
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.2781456953642384,
        "acc_stderr,none": 0.036586032627637426
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.5740740740740741,
        "acc_stderr,none": 0.03372343271653063
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.5089285714285714,
        "acc_stderr,none": 0.04745033255489123
    }
}