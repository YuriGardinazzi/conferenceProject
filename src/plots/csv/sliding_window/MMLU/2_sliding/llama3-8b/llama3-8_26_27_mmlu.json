{
    "mmlu": {
        "acc,none": 0.6512605042016807,
        "acc_stderr,none": 0.0037909722919652667,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.59596174282678,
        "acc_stderr,none": 0.006755066971580248,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.47619047619047616,
        "acc_stderr,none": 0.04467062628403273
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7636363636363637,
        "acc_stderr,none": 0.03317505930009181
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8382352941176471,
        "acc_stderr,none": 0.02584501798692693
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.8270042194092827,
        "acc_stderr,none": 0.024621562866768445
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8429752066115702,
        "acc_stderr,none": 0.03321244842547128
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7685185185185185,
        "acc_stderr,none": 0.04077494709252626
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7300613496932515,
        "acc_stderr,none": 0.034878251684978906
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.7254335260115607,
        "acc_stderr,none": 0.02402774515526501
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.4212290502793296,
        "acc_stderr,none": 0.016513676031179595
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.7331189710610932,
        "acc_stderr,none": 0.025122637608816643
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7314814814814815,
        "acc_stderr,none": 0.024659685185967284
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4641460234680574,
        "acc_stderr,none": 0.012737361318730581
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8304093567251462,
        "acc_stderr,none": 0.028782108105401712
    },
    "mmlu_other": {
        "acc,none": 0.7296427421950434,
        "acc_stderr,none": 0.007625590896827663,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.63,
        "acc_stderr,none": 0.04852365870939099
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7660377358490567,
        "acc_stderr,none": 0.02605529690115292
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6589595375722543,
        "acc_stderr,none": 0.036146654241808254
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.33,
        "acc_stderr,none": 0.047258156262526045
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.7174887892376681,
        "acc_stderr,none": 0.030216831011508755
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8737864077669902,
        "acc_stderr,none": 0.03288180278808628
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8803418803418803,
        "acc_stderr,none": 0.02126271940040697
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.77,
        "acc_stderr,none": 0.04229525846816505
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8365261813537676,
        "acc_stderr,none": 0.013223928616741636
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7647058823529411,
        "acc_stderr,none": 0.024288619466046112
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.5070921985815603,
        "acc_stderr,none": 0.02982449855912901
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.7205882352941176,
        "acc_stderr,none": 0.02725720260611495
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5602409638554217,
        "acc_stderr,none": 0.03864139923699121
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7562560935976601,
        "acc_stderr,none": 0.007567681980583418,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.5087719298245614,
        "acc_stderr,none": 0.047028804320496165
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.8181818181818182,
        "acc_stderr,none": 0.02747960301053878
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8911917098445595,
        "acc_stderr,none": 0.022473253332768763
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6256410256410256,
        "acc_stderr,none": 0.024537591572830506
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.7142857142857143,
        "acc_stderr,none": 0.029344572500634353
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8422018348623853,
        "acc_stderr,none": 0.015630022970092458
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7557251908396947,
        "acc_stderr,none": 0.03768335959728745
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.7254901960784313,
        "acc_stderr,none": 0.018054027458815194
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7,
        "acc_stderr,none": 0.04389311454644287
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7387755102040816,
        "acc_stderr,none": 0.028123429335142797
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8606965174129353,
        "acc_stderr,none": 0.02448448716291397
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.88,
        "acc_stderr,none": 0.03265986323710906
    },
    "mmlu_stem": {
        "acc,none": 0.5540754836663495,
        "acc_stderr,none": 0.008510966646771962,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.26,
        "acc_stderr,none": 0.044084400227680794
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.7111111111111111,
        "acc_stderr,none": 0.03915450630414251
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6842105263157895,
        "acc_stderr,none": 0.0378272898086547
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7569444444444444,
        "acc_stderr,none": 0.03586879280080342
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.49,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.53,
        "acc_stderr,none": 0.05016135580465919
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.33,
        "acc_stderr,none": 0.04725815626252604
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.5098039215686274,
        "acc_stderr,none": 0.04974229460422817
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036846
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5914893617021276,
        "acc_stderr,none": 0.032134180267015755
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6551724137931034,
        "acc_stderr,none": 0.03960933549451207
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.42592592592592593,
        "acc_stderr,none": 0.02546714904546955
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7612903225806451,
        "acc_stderr,none": 0.024251071262208834
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5270935960591133,
        "acc_stderr,none": 0.03512819077876106
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.66,
        "acc_stderr,none": 0.04760952285695237
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.4111111111111111,
        "acc_stderr,none": 0.029999923508706682
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.44370860927152317,
        "acc_stderr,none": 0.04056527902281732
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.5,
        "acc_stderr,none": 0.034099716973523674
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.49107142857142855,
        "acc_stderr,none": 0.04745033255489123
    }
}