{
    "mmlu": {
        "acc,none": 0.5943597778094288,
        "acc_stderr,none": 0.003907695932870597,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5500531349628055,
        "acc_stderr,none": 0.006901980639826923,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.35714285714285715,
        "acc_stderr,none": 0.04285714285714281
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.6787878787878788,
        "acc_stderr,none": 0.03646204963253811
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7352941176470589,
        "acc_stderr,none": 0.03096451792692341
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7637130801687764,
        "acc_stderr,none": 0.027652153144159246
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8429752066115702,
        "acc_stderr,none": 0.03321244842547128
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.6481481481481481,
        "acc_stderr,none": 0.04616631111801713
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7116564417177914,
        "acc_stderr,none": 0.03559039531617342
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.6676300578034682,
        "acc_stderr,none": 0.025361168749688225
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.41564245810055866,
        "acc_stderr,none": 0.016482782187500673
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.662379421221865,
        "acc_stderr,none": 0.026858825879488547
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7129629629629629,
        "acc_stderr,none": 0.025171041915309684
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.41460234680573665,
        "acc_stderr,none": 0.012582597058908284
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7953216374269005,
        "acc_stderr,none": 0.030944459778533204
    },
    "mmlu_other": {
        "acc,none": 0.6704216285806244,
        "acc_stderr,none": 0.00803582525743046,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.59,
        "acc_stderr,none": 0.04943110704237101
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6528301886792452,
        "acc_stderr,none": 0.029300101705549645
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5722543352601156,
        "acc_stderr,none": 0.03772446857518027
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.33,
        "acc_stderr,none": 0.047258156262526045
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6816143497757847,
        "acc_stderr,none": 0.03126580522513713
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8058252427184466,
        "acc_stderr,none": 0.03916667762822583
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8290598290598291,
        "acc_stderr,none": 0.024662496845209807
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.78,
        "acc_stderr,none": 0.04163331998932261
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8199233716475096,
        "acc_stderr,none": 0.013740797258579825
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.6993464052287581,
        "acc_stderr,none": 0.02625605383571896
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.41843971631205673,
        "acc_stderr,none": 0.029427994039419998
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5367647058823529,
        "acc_stderr,none": 0.03029061918048569
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5542168674698795,
        "acc_stderr,none": 0.03869543323472101
    },
    "mmlu_social_sciences": {
        "acc,none": 0.699057523561911,
        "acc_stderr,none": 0.008028603951631012,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.40350877192982454,
        "acc_stderr,none": 0.04615186962583703
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7929292929292929,
        "acc_stderr,none": 0.028869778460267063
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8652849740932642,
        "acc_stderr,none": 0.024639789097709437
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5435897435897435,
        "acc_stderr,none": 0.0252544854247996
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.5714285714285714,
        "acc_stderr,none": 0.032145368597886394
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7798165137614679,
        "acc_stderr,none": 0.01776597865232756
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7251908396946565,
        "acc_stderr,none": 0.039153454088478354
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6813725490196079,
        "acc_stderr,none": 0.01885008469646871
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6545454545454545,
        "acc_stderr,none": 0.04554619617541054
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7142857142857143,
        "acc_stderr,none": 0.02892058322067558
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8208955223880597,
        "acc_stderr,none": 0.027113286753111837
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.84,
        "acc_stderr,none": 0.036845294917747066
    },
    "mmlu_stem": {
        "acc,none": 0.483349191246432,
        "acc_stderr,none": 0.008526504845439662,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6296296296296297,
        "acc_stderr,none": 0.04171654161354543
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6578947368421053,
        "acc_stderr,none": 0.03860731599316092
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7013888888888888,
        "acc_stderr,none": 0.03827052357950756
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.37,
        "acc_stderr,none": 0.04852365870939098
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.45,
        "acc_stderr,none": 0.05
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.34,
        "acc_stderr,none": 0.047609522856952344
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.4019607843137255,
        "acc_stderr,none": 0.048786087144669955
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.78,
        "acc_stderr,none": 0.04163331998932261
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.4595744680851064,
        "acc_stderr,none": 0.032579014820998356
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.5310344827586206,
        "acc_stderr,none": 0.04158632762097828
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.3994708994708995,
        "acc_stderr,none": 0.02522545028406788
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7064516129032258,
        "acc_stderr,none": 0.02590608702131929
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.4236453201970443,
        "acc_stderr,none": 0.03476725747649037
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.31851851851851853,
        "acc_stderr,none": 0.028406533090608466
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.32450331125827814,
        "acc_stderr,none": 0.03822746937658753
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.37962962962962965,
        "acc_stderr,none": 0.033096825811190354
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.4732142857142857,
        "acc_stderr,none": 0.047389751192741546
    }
}