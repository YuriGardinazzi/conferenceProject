{
    "mmlu": {
        "acc,none": 0.6510468594217348,
        "acc_stderr,none": 0.003791426612534463,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5910733262486716,
        "acc_stderr,none": 0.00674736224223163,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.4365079365079365,
        "acc_stderr,none": 0.04435932892851466
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7757575757575758,
        "acc_stderr,none": 0.03256866661681102
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8333333333333334,
        "acc_stderr,none": 0.026156867523931062
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.8270042194092827,
        "acc_stderr,none": 0.024621562866768445
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8429752066115702,
        "acc_stderr,none": 0.03321244842547128
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7592592592592593,
        "acc_stderr,none": 0.04133119440243839
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7116564417177914,
        "acc_stderr,none": 0.035590395316173425
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.7254335260115607,
        "acc_stderr,none": 0.02402774515526501
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.4,
        "acc_stderr,none": 0.01638463841038082
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.7331189710610932,
        "acc_stderr,none": 0.025122637608816646
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7407407407407407,
        "acc_stderr,none": 0.02438366553103545
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4661016949152542,
        "acc_stderr,none": 0.012740853872949834
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8187134502923976,
        "acc_stderr,none": 0.029547741687640038
    },
    "mmlu_other": {
        "acc,none": 0.7219182491149019,
        "acc_stderr,none": 0.007679227192368613,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.62,
        "acc_stderr,none": 0.048783173121456316
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7547169811320755,
        "acc_stderr,none": 0.02648035717989568
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6473988439306358,
        "acc_stderr,none": 0.036430371689585475
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145632
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.7130044843049327,
        "acc_stderr,none": 0.030360379710291954
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8737864077669902,
        "acc_stderr,none": 0.03288180278808628
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8846153846153846,
        "acc_stderr,none": 0.020930193185179333
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.78,
        "acc_stderr,none": 0.041633319989322605
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8288633461047255,
        "acc_stderr,none": 0.013468201614066325
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7647058823529411,
        "acc_stderr,none": 0.024288619466046112
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4574468085106383,
        "acc_stderr,none": 0.029719281272236837
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.7095588235294118,
        "acc_stderr,none": 0.027576468622740512
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5542168674698795,
        "acc_stderr,none": 0.03869543323472101
    },
    "mmlu_social_sciences": {
        "acc,none": 0.758856028599285,
        "acc_stderr,none": 0.007562820078483319,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.5087719298245614,
        "acc_stderr,none": 0.04702880432049615
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.8080808080808081,
        "acc_stderr,none": 0.028057791672989017
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.9067357512953368,
        "acc_stderr,none": 0.020986854593289708
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6538461538461539,
        "acc_stderr,none": 0.02412112541694119
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.726890756302521,
        "acc_stderr,none": 0.028942004040998164
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8348623853211009,
        "acc_stderr,none": 0.01591955782997604
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7633587786259542,
        "acc_stderr,none": 0.03727673575596915
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.7189542483660131,
        "acc_stderr,none": 0.018185218954318082
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7181818181818181,
        "acc_stderr,none": 0.04309118709946458
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.7428571428571429,
        "acc_stderr,none": 0.027979823538744546
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8557213930348259,
        "acc_stderr,none": 0.02484575321230605
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.86,
        "acc_stderr,none": 0.0348735088019777
    },
    "mmlu_stem": {
        "acc,none": 0.5654931810973676,
        "acc_stderr,none": 0.00848581430720893,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.34,
        "acc_stderr,none": 0.047609522856952365
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6888888888888889,
        "acc_stderr,none": 0.039992628766177214
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.7302631578947368,
        "acc_stderr,none": 0.03611780560284898
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7777777777777778,
        "acc_stderr,none": 0.03476590104304135
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.45,
        "acc_stderr,none": 0.04999999999999999
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.52,
        "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.37,
        "acc_stderr,none": 0.048523658709390974
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.5098039215686274,
        "acc_stderr,none": 0.04974229460422817
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036847
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5872340425531914,
        "acc_stderr,none": 0.03218471141400351
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6689655172413793,
        "acc_stderr,none": 0.03921545312467122
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.42328042328042326,
        "acc_stderr,none": 0.025446365634406783
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7774193548387097,
        "acc_stderr,none": 0.023664216671642525
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5369458128078818,
        "acc_stderr,none": 0.035083705204426656
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.04725815626252607
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.4,
        "acc_stderr,none": 0.029869605095316904
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.46357615894039733,
        "acc_stderr,none": 0.04071636065944216
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.5694444444444444,
        "acc_stderr,none": 0.03376922151252335
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.48214285714285715,
        "acc_stderr,none": 0.047427623612430116
    }
}