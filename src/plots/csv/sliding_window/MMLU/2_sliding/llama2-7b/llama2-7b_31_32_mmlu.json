{
    "mmlu": {
        "acc,none": 0.435336846603048,
        "acc_stderr,none": 0.004068384473845856,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.40913921360255046,
        "acc_stderr,none": 0.006948770383009474,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.38095238095238093,
        "acc_stderr,none": 0.04343525428949097
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.593939393939394,
        "acc_stderr,none": 0.03834816355401181
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.45588235294117646,
        "acc_stderr,none": 0.034956245220154725
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.5232067510548524,
        "acc_stderr,none": 0.03251215201141018
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.6363636363636364,
        "acc_stderr,none": 0.043913262867240704
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.46296296296296297,
        "acc_stderr,none": 0.04820403072760627
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.50920245398773,
        "acc_stderr,none": 0.03927705600787443
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.4653179190751445,
        "acc_stderr,none": 0.026854257928258893
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.24581005586592178,
        "acc_stderr,none": 0.014400296429225594
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.5562700964630225,
        "acc_stderr,none": 0.02821768355665232
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.4567901234567901,
        "acc_stderr,none": 0.02771666165019404
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.34485006518904826,
        "acc_stderr,none": 0.012139881006287052
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7076023391812866,
        "acc_stderr,none": 0.03488647713457921
    },
    "mmlu_other": {
        "acc,none": 0.5027357579658834,
        "acc_stderr,none": 0.008784032559622623,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.49,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.44528301886792454,
        "acc_stderr,none": 0.030588052974270655
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.42196531791907516,
        "acc_stderr,none": 0.037657466938651483
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542128
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.5112107623318386,
        "acc_stderr,none": 0.033549366530984746
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.4854368932038835,
        "acc_stderr,none": 0.04948637324026637
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.6794871794871795,
        "acc_stderr,none": 0.030572811310299604
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.51,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.611749680715198,
        "acc_stderr,none": 0.017427673295544337
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.4934640522875817,
        "acc_stderr,none": 0.028627470550556054
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.3262411347517731,
        "acc_stderr,none": 0.027968453043563168
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.4963235294117647,
        "acc_stderr,none": 0.030372015885428195
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.3795180722891566,
        "acc_stderr,none": 0.037777988227480165
    },
    "mmlu_social_sciences": {
        "acc,none": 0.49171270718232046,
        "acc_stderr,none": 0.008882651143497207,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.2543859649122807,
        "acc_stderr,none": 0.040969851398436716
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.4898989898989899,
        "acc_stderr,none": 0.035616254886737454
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.6683937823834197,
        "acc_stderr,none": 0.03397636541089118
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.4282051282051282,
        "acc_stderr,none": 0.025088301454694834
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.42436974789915966,
        "acc_stderr,none": 0.03210479051015776
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.5743119266055046,
        "acc_stderr,none": 0.0211992359724708
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.5190839694656488,
        "acc_stderr,none": 0.04382094705550988
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.4199346405228758,
        "acc_stderr,none": 0.019966811178256483
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.5,
        "acc_stderr,none": 0.04789131426105757
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.4897959183673469,
        "acc_stderr,none": 0.03200255347893782
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.5970149253731343,
        "acc_stderr,none": 0.034683432951111266
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.57,
        "acc_stderr,none": 0.04975698519562428
    },
    "mmlu_stem": {
        "acc,none": 0.35299714557564227,
        "acc_stderr,none": 0.008408142282263347,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.29,
        "acc_stderr,none": 0.04560480215720684
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.42962962962962964,
        "acc_stderr,none": 0.04276349494376599
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.3881578947368421,
        "acc_stderr,none": 0.03965842097512744
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.4375,
        "acc_stderr,none": 0.04148415739394154
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.36,
        "acc_stderr,none": 0.04824181513244218
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145633
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.34,
        "acc_stderr,none": 0.04760952285695235
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.22549019607843138,
        "acc_stderr,none": 0.041583075330832865
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.55,
        "acc_stderr,none": 0.05
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.43829787234042555,
        "acc_stderr,none": 0.03243618636108102
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.41379310344827586,
        "acc_stderr,none": 0.041042692118062316
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.2619047619047619,
        "acc_stderr,none": 0.022644212615525214
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.45483870967741935,
        "acc_stderr,none": 0.028327743091561063
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.29064039408866993,
        "acc_stderr,none": 0.031947400722655395
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.38,
        "acc_stderr,none": 0.048783173121456344
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.2518518518518518,
        "acc_stderr,none": 0.026466117538959916
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.2781456953642384,
        "acc_stderr,none": 0.03658603262763743
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.3101851851851852,
        "acc_stderr,none": 0.03154696285656628
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.36607142857142855,
        "acc_stderr,none": 0.0457237235873743
    }
}