{
    "mmlu": {
        "acc,none": 0.6338128471727674,
        "acc_stderr,none": 0.003798444634154653,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5749202975557917,
        "acc_stderr,none": 0.006713866653392654,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.4444444444444444,
        "acc_stderr,none": 0.04444444444444449
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7515151515151515,
        "acc_stderr,none": 0.033744026441394036
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.8333333333333334,
        "acc_stderr,none": 0.026156867523931055
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.8143459915611815,
        "acc_stderr,none": 0.02531049537694487
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.8429752066115702,
        "acc_stderr,none": 0.03321244842547128
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7407407407407407,
        "acc_stderr,none": 0.04236511258094633
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.754601226993865,
        "acc_stderr,none": 0.03380939813943354
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.7138728323699421,
        "acc_stderr,none": 0.024332146779134128
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3407821229050279,
        "acc_stderr,none": 0.015852002449862113
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.7459807073954984,
        "acc_stderr,none": 0.024723861504771683
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7191358024691358,
        "acc_stderr,none": 0.025006469755799208
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.45632333767926986,
        "acc_stderr,none": 0.012721420501462546
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8187134502923976,
        "acc_stderr,none": 0.029547741687640038
    },
    "mmlu_other": {
        "acc,none": 0.7196652719665272,
        "acc_stderr,none": 0.0076939537859211215,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.64,
        "acc_stderr,none": 0.048241815132442176
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.7509433962264151,
        "acc_stderr,none": 0.02661648298050171
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5895953757225434,
        "acc_stderr,none": 0.03750757044895537
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.38,
        "acc_stderr,none": 0.048783173121456316
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.7219730941704036,
        "acc_stderr,none": 0.030069584874494047
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.883495145631068,
        "acc_stderr,none": 0.031766839486404075
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8846153846153846,
        "acc_stderr,none": 0.020930193185179333
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.81,
        "acc_stderr,none": 0.03942772444036622
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8301404853128991,
        "acc_stderr,none": 0.013428186370608289
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7647058823529411,
        "acc_stderr,none": 0.024288619466046112
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.48226950354609927,
        "acc_stderr,none": 0.02980873964223777
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.6801470588235294,
        "acc_stderr,none": 0.028332959514031232
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5301204819277109,
        "acc_stderr,none": 0.03885425420866766
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7435814104647384,
        "acc_stderr,none": 0.007681415261291205,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.49122807017543857,
        "acc_stderr,none": 0.04702880432049615
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.8131313131313131,
        "acc_stderr,none": 0.027772533334218984
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8704663212435233,
        "acc_stderr,none": 0.024233532297758716
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5948717948717949,
        "acc_stderr,none": 0.024890471769938142
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6974789915966386,
        "acc_stderr,none": 0.029837962388291932
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8238532110091743,
        "acc_stderr,none": 0.01633288239343138
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7633587786259542,
        "acc_stderr,none": 0.037276735755969154
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.7026143790849673,
        "acc_stderr,none": 0.01849259653639695
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7272727272727273,
        "acc_stderr,none": 0.04265792110940589
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.746938775510204,
        "acc_stderr,none": 0.027833023871399694
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8656716417910447,
        "acc_stderr,none": 0.024112678240900826
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.89,
        "acc_stderr,none": 0.031446603773522014
    },
    "mmlu_stem": {
        "acc,none": 0.5299714557564225,
        "acc_stderr,none": 0.00849289191476491,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.29,
        "acc_stderr,none": 0.045604802157206824
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6962962962962963,
        "acc_stderr,none": 0.039725528847851375
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6578947368421053,
        "acc_stderr,none": 0.03860731599316092
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7638888888888888,
        "acc_stderr,none": 0.03551446610810826
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.46,
        "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.42,
        "acc_stderr,none": 0.049604496374885836
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.32,
        "acc_stderr,none": 0.046882617226215034
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.45098039215686275,
        "acc_stderr,none": 0.04951218252396262
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.82,
        "acc_stderr,none": 0.038612291966536934
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5872340425531914,
        "acc_stderr,none": 0.03218471141400351
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6482758620689655,
        "acc_stderr,none": 0.03979236637497409
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.4074074074074074,
        "acc_stderr,none": 0.025305906241590632
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7483870967741936,
        "acc_stderr,none": 0.024685979286239956
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5123152709359606,
        "acc_stderr,none": 0.035169204442208966
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.64,
        "acc_stderr,none": 0.04824181513244218
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.34814814814814815,
        "acc_stderr,none": 0.02904560029061626
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.4105960264900662,
        "acc_stderr,none": 0.04016689594849928
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.4537037037037037,
        "acc_stderr,none": 0.03395322726375798
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.44642857142857145,
        "acc_stderr,none": 0.04718471485219588
    }
}