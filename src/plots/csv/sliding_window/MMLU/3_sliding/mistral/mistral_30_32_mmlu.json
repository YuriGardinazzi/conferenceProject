{
    "mmlu": {
        "acc,none": 0.6166500498504487,
        "acc_stderr,none": 0.0038552736255633886,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.5625929861849097,
        "acc_stderr,none": 0.006748128463470164,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.38095238095238093,
        "acc_stderr,none": 0.043435254289490965
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7575757575757576,
        "acc_stderr,none": 0.03346409881055953
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7990196078431373,
        "acc_stderr,none": 0.02812597226565437
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7805907172995781,
        "acc_stderr,none": 0.026939106581553945
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.7851239669421488,
        "acc_stderr,none": 0.03749492448709699
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7777777777777778,
        "acc_stderr,none": 0.040191074725573483
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7730061349693251,
        "acc_stderr,none": 0.03291099578615769
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.708092485549133,
        "acc_stderr,none": 0.024476994076247316
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3240223463687151,
        "acc_stderr,none": 0.01565254249642112
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.6945337620578779,
        "acc_stderr,none": 0.026160584450140457
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.7129629629629629,
        "acc_stderr,none": 0.02517104191530968
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.45436766623207303,
        "acc_stderr,none": 0.012716941720734802
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.8304093567251462,
        "acc_stderr,none": 0.02878210810540171
    },
    "mmlu_other": {
        "acc,none": 0.6977792082394593,
        "acc_stderr,none": 0.007948963263603738,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.59,
        "acc_stderr,none": 0.049431107042371025
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6754716981132075,
        "acc_stderr,none": 0.028815615713432115
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5953757225433526,
        "acc_stderr,none": 0.03742461193887249
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001975
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.695067264573991,
        "acc_stderr,none": 0.03089861088247752
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.8058252427184466,
        "acc_stderr,none": 0.03916667762822584
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8803418803418803,
        "acc_stderr,none": 0.021262719400406957
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.69,
        "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.8084291187739464,
        "acc_stderr,none": 0.014072859310451949
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.7647058823529411,
        "acc_stderr,none": 0.024288619466046123
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.5,
        "acc_stderr,none": 0.029827499313594685
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.6433823529411765,
        "acc_stderr,none": 0.029097209568411962
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.5542168674698795,
        "acc_stderr,none": 0.03869543323472101
    },
    "mmlu_social_sciences": {
        "acc,none": 0.7227819304517387,
        "acc_stderr,none": 0.007901635364839385,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.47368421052631576,
        "acc_stderr,none": 0.046970851366478626
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7828282828282829,
        "acc_stderr,none": 0.029376616484945644
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.8497409326424871,
        "acc_stderr,none": 0.025787723180723886
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.6307692307692307,
        "acc_stderr,none": 0.02446861524147892
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6764705882352942,
        "acc_stderr,none": 0.030388353551886793
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.8238532110091743,
        "acc_stderr,none": 0.016332882393431395
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7633587786259542,
        "acc_stderr,none": 0.03727673575596915
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6552287581699346,
        "acc_stderr,none": 0.019228322018696647
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6363636363636364,
        "acc_stderr,none": 0.046075820907199756
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.710204081632653,
        "acc_stderr,none": 0.029043088683304345
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.8208955223880597,
        "acc_stderr,none": 0.027113286753111837
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.85,
        "acc_stderr,none": 0.03588702812826369
    },
    "mmlu_stem": {
        "acc,none": 0.5137963843958135,
        "acc_stderr,none": 0.008519785369573851,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542126
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.6074074074074074,
        "acc_stderr,none": 0.04218506215368879
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.6842105263157895,
        "acc_stderr,none": 0.03782728980865469
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7152777777777778,
        "acc_stderr,none": 0.03773809990686935
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.49,
        "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.48,
        "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145633
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.3627450980392157,
        "acc_stderr,none": 0.04784060704105654
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.74,
        "acc_stderr,none": 0.0440844002276808
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5787234042553191,
        "acc_stderr,none": 0.03227834510146268
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.5793103448275863,
        "acc_stderr,none": 0.0411391498118926
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.3862433862433862,
        "acc_stderr,none": 0.025075981767601684
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7451612903225806,
        "acc_stderr,none": 0.024790118459332204
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5073891625615764,
        "acc_stderr,none": 0.0351760354036101
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.67,
        "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.3074074074074074,
        "acc_stderr,none": 0.028133252578815642
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.32450331125827814,
        "acc_stderr,none": 0.03822746937658754
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.47685185185185186,
        "acc_stderr,none": 0.03406315360711507
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.49107142857142855,
        "acc_stderr,none": 0.04745033255489123
    }
}