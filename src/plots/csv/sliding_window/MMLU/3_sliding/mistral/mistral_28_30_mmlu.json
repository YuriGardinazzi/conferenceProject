{
    "mmlu": {
        "acc,none": 0.2519584104828372,
        "acc_stderr,none": 0.0036094955263800115,
        "alias": "mmlu"
    },
    "mmlu_humanities": {
        "acc,none": 0.22933049946865036,
        "acc_stderr,none": 0.006057015895999265,
        "alias": " - humanities"
    },
    "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.25396825396825395,
        "acc_stderr,none": 0.03893259610604673
    },
    "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.18787878787878787,
        "acc_stderr,none": 0.03050193405942914
    },
    "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.1568627450980392,
        "acc_stderr,none": 0.025524722324553332
    },
    "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.19831223628691982,
        "acc_stderr,none": 0.025955020841621112
    },
    "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.2066115702479339,
        "acc_stderr,none": 0.03695980128098824
    },
    "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.1388888888888889,
        "acc_stderr,none": 0.033432700628696216
    },
    "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.2392638036809816,
        "acc_stderr,none": 0.0335195387952127
    },
    "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.23121387283236994,
        "acc_stderr,none": 0.022698657167855706
    },
    "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.2335195530726257,
        "acc_stderr,none": 0.01414957534897626
    },
    "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.3183279742765273,
        "acc_stderr,none": 0.026457225067811032
    },
    "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.2808641975308642,
        "acc_stderr,none": 0.02500646975579922
    },
    "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.19035202086049544,
        "acc_stderr,none": 0.010026648414002484
    },
    "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.5087719298245614,
        "acc_stderr,none": 0.038342347441649924
    },
    "mmlu_other": {
        "acc,none": 0.28966849050531057,
        "acc_stderr,none": 0.00791397911484313,
        "alias": " - other"
    },
    "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.28,
        "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.18490566037735848,
        "acc_stderr,none": 0.023893351834464314
    },
    "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.2138728323699422,
        "acc_stderr,none": 0.03126511206173042
    },
    "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.26,
        "acc_stderr,none": 0.0440844002276808
    },
    "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.21524663677130046,
        "acc_stderr,none": 0.027584066602208274
    },
    "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.44660194174757284,
        "acc_stderr,none": 0.04922424153458933
    },
    "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.33760683760683763,
        "acc_stderr,none": 0.030980296992618558
    },
    "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.25,
        "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.4521072796934866,
        "acc_stderr,none": 0.017797751493865626
    },
    "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.21568627450980393,
        "acc_stderr,none": 0.02355083135199509
    },
    "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.20921985815602837,
        "acc_stderr,none": 0.02426476943998846
    },
    "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.17279411764705882,
        "acc_stderr,none": 0.0229660675855818
    },
    "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.21686746987951808,
        "acc_stderr,none": 0.03208284450356365
    },
    "mmlu_social_sciences": {
        "acc,none": 0.27624309392265195,
        "acc_stderr,none": 0.008022616790446083,
        "alias": " - social sciences"
    },
    "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.2631578947368421,
        "acc_stderr,none": 0.0414243971948936
    },
    "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.3383838383838384,
        "acc_stderr,none": 0.03371124142626302
    },
    "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.32642487046632124,
        "acc_stderr,none": 0.033840286211432945
    },
    "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.26666666666666666,
        "acc_stderr,none": 0.022421273612923714
    },
    "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.24789915966386555,
        "acc_stderr,none": 0.028047967224176892
    },
    "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.3192660550458716,
        "acc_stderr,none": 0.019987829069750017
    },
    "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.3053435114503817,
        "acc_stderr,none": 0.04039314978724561
    },
    "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.21405228758169934,
        "acc_stderr,none": 0.016593429662329035
    },
    "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.22727272727272727,
        "acc_stderr,none": 0.04013964554072773
    },
    "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.20816326530612245,
        "acc_stderr,none": 0.025991117672813296
    },
    "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.36318407960199006,
        "acc_stderr,none": 0.03400598505599015
    },
    "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.33,
        "acc_stderr,none": 0.04725815626252605
    },
    "mmlu_stem": {
        "acc,none": 0.2248652077386616,
        "acc_stderr,none": 0.007389040774146307,
        "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.22,
        "acc_stderr,none": 0.041633319989322695
    },
    "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.3333333333333333,
        "acc_stderr,none": 0.04072314811876837
    },
    "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.34868421052631576,
        "acc_stderr,none": 0.03878139888797611
    },
    "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.2916666666666667,
        "acc_stderr,none": 0.03800968060554859
    },
    "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.23,
        "acc_stderr,none": 0.04229525846816506
    },
    "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.19,
        "acc_stderr,none": 0.03942772444036623
    },
    "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.21,
        "acc_stderr,none": 0.040936018074033256
    },
    "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.2549019607843137,
        "acc_stderr,none": 0.043364327079931785
    },
    "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.34,
        "acc_stderr,none": 0.047609522856952365
    },
    "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.2,
        "acc_stderr,none": 0.0261488180184245
    },
    "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.19310344827586207,
        "acc_stderr,none": 0.03289445522127401
    },
    "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.18253968253968253,
        "acc_stderr,none": 0.01989487936717554
    },
    "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.2129032258064516,
        "acc_stderr,none": 0.023287665127268535
    },
    "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.2955665024630542,
        "acc_stderr,none": 0.032104944337514575
    },
    "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.2,
        "acc_stderr,none": 0.04020151261036844
    },
    "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.17407407407407408,
        "acc_stderr,none": 0.023118596033551847
    },
    "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.152317880794702,
        "acc_stderr,none": 0.0293390688314987
    },
    "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.18518518518518517,
        "acc_stderr,none": 0.026491914727355164
    },
    "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.21428571428571427,
        "acc_stderr,none": 0.038946411200447915
    }
}